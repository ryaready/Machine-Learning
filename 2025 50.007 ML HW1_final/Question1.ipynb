{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.1.1\n",
    " Please provide definiton of Hinge Loss Function with the offset $\\theta_0$***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge loss: \n",
    "\n",
    "$Loss_h(z) = max{1- y (\\theta^Tx), 0}$\n",
    "\n",
    "Hinge loss with offset $\\theta_0$: \n",
    "\n",
    "$Loss_h(z) = max{1- y (\\theta^T x + \\theta_0), 0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.1.2 Describe how to update weights $\\theta$ and $\\theta_0$ to minimize the hinge loss function using stochastic sub-gradient descent***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is an optimization method used to minimize a loss funciton like the hinge loss function by updating parameters using randomly selected data points. For hinge loss, the goal is to maximize the margin between data points and the decision boundary.\n",
    "\n",
    "Hinge loss can be calculated as: $Loss_h(z) = max{1- y (\\theta^Tx), 0}$. If $yt(\\theta \\cdot  x_t) >= 1$ the point is correctly classified and the loss is 0. However if $yt(\\theta \\cdot  x_t) < 1$ the point is either within the margin or misclassified, and hence the loss is positive. \n",
    "\n",
    "To optimize the classifier, we can compute the gradient of hinge loss with respect to \\theta. If $yt(\\theta \\cdot  x_t) >= 1$, then no update is needed. If $yt(\\theta \\cdot  x_t) < 1$ then the weight vector must be updated in the direction of correction.\n",
    "\n",
    "SGD updates the weight vector $\\theta$ for each randomly selected sample $(xt, yt)$: \n",
    "$\\theta^{(k + 1)} = \\theta^{(k)} - \\eta_k\\nabla\\theta Loss$\n",
    "\n",
    "In SGD, $\\theta$ is initialized as 0. Then a random traning example is seleted and the hinge loss is calculated. If $yt(\\theta \\cdot  x_t) >= 1$, then no update is needed. If $yt(\\theta \\cdot  x_t) < 1$ then we compute the gradient of the hinge loss: $\\nabla_\\theta Loss = -ytxt$. This tells us the direction in which we need to adjust $\\theta$. $\\theta = \\theta - \\eta_k(-ytxt)$ therefore $\\theta = \\theta + \\eta_k ytxt$. Where $\\eta_k$ is the learning rate. This is repeated until the max number of iterations is reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, y, loss_gradient, lr = 0.01, max_iter = 100): \n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "\n",
    "    X: numpy array of shape (n_samples, n_features) - features \n",
    "    y: numpy array of shape (n_samples,) - target values\n",
    "    loss_gradient: function that computes the gradient of the loss function\n",
    "    lr: learning rate\n",
    "    max_iter: number of iterations\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    theta = np.zeros(n_features) # initialze theta to 0 \n",
    "\n",
    "    for k in range(max_iter):\n",
    "        t = np.random.randing(n_samples)\n",
    "        grad = loss_gradient(X[t], y[t], theta)\n",
    "        theta -= lr * grad \n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_gradient(x_t, y_t, theta):\n",
    "    if y_t * np.dot(x_t, theta) < 1:\n",
    "        return -y_t * x_t\n",
    "\n",
    "    return np.zeros_like(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.2.1 Implement the Perceptron Algorithm of Linear Classification with offset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_with_offset(theta, offset, feature, target, num_features, max_iter= 100): \n",
    "    it = 0\n",
    "    error = 0\n",
    "    while it <= max_iter: \n",
    "        m = 0 \n",
    "        for i in range(0,len(target)): \n",
    "            if target[i]*(np.dot(theta, (feature[i]+ offset))) <= 0: \n",
    "                theta = theta + (feature[i] * target[i])\n",
    "                offset = offset + target[i]\n",
    "                error += 1\n",
    "\n",
    "                m += 1\n",
    "                it +=1\n",
    "        if m == 0: \n",
    "            break\n",
    "\n",
    "        err = error/len(target)\n",
    "        ls = [theta, offset, err]\n",
    "\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.2.2 Train Linear Classifier with offset using Perceptron Algorithm and Evaluate***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data/hw1_train_1_5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.iloc[:, :2].to_numpy()\n",
    "y = training_data.iloc[:, 2].values\n",
    "\n",
    "num_features = X.shape[1]\n",
    "num_epochs = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned weightd: [-439.74367432   51.72785564]\n",
      "learned bias: 621.0\n",
      "training error: 0\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(num_features) * 0.01 \n",
    "offset = 0.0\n",
    "error = 0\n",
    "num_epochs = 1\n",
    "\n",
    "for i in range(num_epochs): \n",
    "    ls = perceptron_with_offset(theta, offset, X, y, num_features)\n",
    "    theta = ls[0]\n",
    "    offset = ls[1]\n",
    "    err = ls[2]\n",
    "    \n",
    "    \n",
    "print('learned weightd:', theta)\n",
    "print('learned bias:', offset)\n",
    "print('training error:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [-2198.79817644   258.64768387]\n",
      "offset: 3105.0\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(num_features) * 0.01 \n",
    "offset = 0.0\n",
    "num_epochs = 5\n",
    "\n",
    "for i in range(num_epochs): \n",
    "    ls = perceptron_with_offset(theta, offset, X, y, num_features)\n",
    "    theta = ls[0]\n",
    "    offset = ls[1]\n",
    "    \n",
    "    \n",
    "print('theta:', theta)\n",
    "print('offset:', offset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
